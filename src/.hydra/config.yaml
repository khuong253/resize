backbone:
  _target_: models.transformer_utils.TransformerEncoder
  encoder_layer:
    _target_: models.transformer_utils.Block
    d_model: 512
    nhead: 8
    dim_feedforward: 1024
    dropout: 0.1
    batch_first: true
    norm_first: true
  num_layers: 4
dataset:
  _target_: datasets.obello.ObelloDataset
  _partial_: true
  dir: ./download/datasets
  max_seq_length: 15
data:
  batch_size: 64
  bbox_quantization: kmeans
  num_bin_bboxes: 128
  num_workers: 4
  pad_until_max: true
  shared_bbox_vocab: x-y-w-h
  special_tokens:
  - pad
  - bos
  - eos
  - sep
  - mask
  transforms: []
  var_order: c-w-h-x-y
model:
  _target_: models.bart.BART
  _partial_: true
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.0001
  betas:
  - 0.9
  - 0.98
sampling:
  temperature: 1.0
  name: random
scheduler:
  _target_: helpers.scheduler.VoidScheduler
  _partial_: true
training:
  resume_epoch: ''
  resume: false
  pretrained: false
  epochs: 10
  grad_norm_clip: 1.0
  weight_decay: 0.1
  saving_epoch_interval: 50
  loss_plot_iter_interval: 50
  sample_plot_epoch_interval: 1
  fid_plot_num_samples: 200
  fid_plot_batch_size: 64
job_dir: ./tmp/bart
fid_weight_dir: ./tmp/fid_weights
seed: 0
device: cuda
debug: false
